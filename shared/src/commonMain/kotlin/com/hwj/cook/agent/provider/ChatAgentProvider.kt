package com.hwj.cook.agent.provider

import ai.koog.agents.core.agent.AIAgent
import ai.koog.agents.core.agent.config.AIAgentConfig
import ai.koog.agents.core.agent.isFinished
import ai.koog.agents.core.agent.isRunning
import ai.koog.agents.core.tools.ToolRegistry
import ai.koog.agents.features.eventHandler.feature.handleEvents
import ai.koog.prompt.dsl.Prompt
import ai.koog.prompt.dsl.prompt
import ai.koog.prompt.message.ContentPart
import ai.koog.prompt.message.Message
import ai.koog.prompt.message.RequestMetaInfo
import ai.koog.prompt.message.ResponseMetaInfo
import ai.koog.prompt.params.LLMParams
import ai.koog.prompt.streaming.StreamFrame
import com.hwj.cook.agent.ChatMsg
import com.hwj.cook.agent.JsonApi
import com.hwj.cook.agent.buildQwen3LLM
import com.hwj.cook.agent.createAiExecutor
import com.hwj.cook.global.DATA_APP_TOKEN
import com.hwj.cook.global.DATA_MCP_KEY
import com.hwj.cook.global.getCacheString
import com.hwj.cook.global.printD
import com.hwj.cook.global.printLog
import kotlin.time.ExperimentalTime

/**
 * @author by jason-ä½•ä¼Ÿæ°ï¼Œ2025/10/11
 * des:é—®ç­”å¯¹è¯åªéœ€è¦ä¼šè¯æ¶ˆæ¯å’Œå¤§æ¨¡å‹æ¥å£å³å¯ï¼Œä¸éœ€è¦å®Œæ•´çš„æ™ºèƒ½ä½“èƒ½åŠ›ï¼Œ
 * è¿™é‡Œå†™ä¸ªç®€å•çš„é—®ç­”æ™ºèƒ½ä½“ç”¨æ¥å®ç°ç®€å•çš„é—®ç­”å¯¹è¯
 */
class ChatAgentProvider(
    override var title: String = "Chat",
    override val description: String = "A conversational agent that supports long-term memory, with clear and concise responses."
) : AgentProvider<String, String> {

    private var agentInstance: AIAgent<String, String>? = null
//    private val msgHistory = mutableListOf<Pair<String, String>>() //ç”¨æˆ·é—®é¢˜-ã€‹æ™ºèƒ½ä½“å›ç­”

    @OptIn(ExperimentalTime::class)
    override suspend fun provideAgent(
        onToolCallEvent: suspend (Message.Tool.Call) -> Unit,
        onToolResultEvent: suspend (Message.Tool.Result) -> Unit,
        onLLMStreamFrameEvent: suspend (String) -> Unit,
        onErrorEvent: suspend (String) -> Unit,
        onAssistantMessage: suspend (String) -> String
    ): AIAgent<String, String> {
        val apiKey = getCacheString(DATA_APP_TOKEN)
        val mcpKey = getCacheString(DATA_MCP_KEY)
        require(apiKey?.isNotEmpty() == true) { "apiKey is not configured." }

        //æ™ºèƒ½ä½“é»˜è®¤éæµå¼å›å¤ï¼Ÿï¼Ÿï¼Ÿ
//        val remoteAiExecutor = SingleLLMPromptExecutor(OpenAiRemoteLLMClient(apiKey))
        val remoteAiExecutor = createAiExecutor(apiKey)

//        DeepSeekLLMClient
//        OpenAILLMClient

//        MultiLLMPromptExecutor () //æ··åˆå®¢æˆ·ç«¯
//        DefaultMultiLLMPromptExecutor()
        val toolRegistry = ToolRegistry {
//            tool(ExitTool)
//            tool(ASearchTool) //endpointæ˜¯ç»™åç«¯æ¥çš„ã€‚ã€‚æˆ‘ç”¨ä¸äº†
        }
//            .plus(McpToolCI.searchSSE2())
//            .plus(McpToolCI.searchSSE3())
//            .plus(McpToolCI.searchSSE4()) //æš‚æ—¶æ²¡æ³•å®ç°å¤šå¹³å°çš„mcpï¼Œå…ˆæ”¾æ”¾
//            .plus(McpToolCI.webParserSSE(mcpKey))
        toolRegistry.tools.forEach { printD(it.name + "ï¼š" + it.descriptor, "Atool>") }
        agentInstance = AIAgent.invoke(
            promptExecutor = remoteAiExecutor,
            toolRegistry = toolRegistry,
//            strategy = ,
            agentConfig = AIAgentConfig(
                prompt = prompt(
                    id = "chat",
                    params = LLMParams(temperature = 0.8, numberOfChoices = 1, maxTokens = 1500)
                ) {
                    system("I'm an assistant who provides simple and clear answers to users.")
//                    system ("""
//You are an assistant that must use tools to answer any question
//that requires real-time or factual information such as dates, news, or web content.
//If a tool is available, you should call it instead of answering directly.
//""")

//                    user {  }
//                    assistant {  }
//                    tool {
//                        call()
//                        result()
//                    }
                },
                model = buildQwen3LLM("Qwen/Qwen2.5-7B-Instruct"),
                maxAgentIterations = 50 //å¤ªå°‘åè€Œä¼šå¯¼è‡´æ— æ³•ç»“æŸæ™ºèƒ½ä½“
            ),
        ) {
            handleEvents {
                onToolCallStarting { ctx ->
//                    onToolCallEvent("\nğŸ”§ Using ${ctx.toolName} with ${ctx.toolArgs}... ")
                    onToolCallEvent(
                        Message.Tool.Call(
                            id = ctx.toolCallId,
                            tool = ctx.toolName,
                            part = ContentPart.Text(text = JsonApi.encodeToString(ctx.toolArgs)),
                            metaInfo = ResponseMetaInfo.Empty //å¯¹ä¸ä¸Šç±»å‹
                        )
                    )
                }
                onToolCallCompleted { ctx ->
                    onToolResultEvent(
                        Message.Tool.Result(
                            id = ctx.toolCallId,
                            tool = ctx.toolName,
                            part = ContentPart.Text(text = JsonApi.encodeToString(ctx.toolArgs)),
                            metaInfo = RequestMetaInfo.Empty
                        )
                    )
                }
                onLLMStreamingFrameReceived { ctx ->
                    when (val chunk = ctx.streamFrame) {
                        is StreamFrame.Append -> {
                            onLLMStreamFrameEvent(chunk.text)
                        }

                        is StreamFrame.ToolCall -> {
                            printLog("Tool call:${chunk.name} args=${chunk.content} ")
                        }

                        is StreamFrame.End -> {
                            printLog("\n[END] reason=${chunk.finishReason}")
                        }
                    }
                }
                onAgentExecutionFailed { ctx ->
                    onErrorEvent("${ctx.throwable.message}")
                }

                onAgentCompleted { ctx ->
//                    ctx.result
                    // Skip finish event handling
                }
            }
        }

        return agentInstance!!
    }

    private fun creatPrompt(
        list: List<ChatMsg>,
        params: LLMParams = LLMParams(
            temperature = 0.8,
            numberOfChoices = 1,
            maxTokens = 1500
        )
    ): Prompt {
        return prompt(id = "2", params = params) {
            list.forEach { msg ->
                when (msg) {
                    is ChatMsg.UserMsg -> user(msg.txt)
                    is ChatMsg.AgentMsg -> assistant(msg.txt)
                    is ChatMsg.SystemMsg -> msg.txt?.let {
                        system(it)//ç•Œé¢ä¸€èˆ¬ä¸æ˜¾ç¤º,è¿™é‡Œæ˜¯æ•°æ®æ„é€ 
                    }

                    is ChatMsg.ErrorMsg -> msg.txt?.let { assistant(it) }
                    is ChatMsg.ToolCallMsg -> {
                        tool { call(msg.call) }
                    }

                    is ChatMsg.ToolResultMsg -> {
                        tool { result(msg.result) }
                    }

                    is ChatMsg.ResultMsg -> assistant(msg.txt)
                }
            }
        }
    }

    //å‡½æ•°å¼è®¾è®¡ç­–ç•¥
//    private fun exportMsgStrategy(): AIAgentFunctionalStrategy<String, Unit> {
//        return functionalStrategy {
//            var userResponse = it
//            while (userResponse != "/bye") {
//                val responses = requestLLM(userResponse)
//                msgHistory += userResponse to responses.content
//                userResponse = readln() //ä¸“ç”¨æ§åˆ¶å°çš„è¾“å…¥
//            }
//        }
//    }

    suspend fun isRunning(): Boolean {
        return agentInstance!!.isRunning()
    }

    suspend fun isFinished(): Boolean {
        return agentInstance!!.isFinished()
    }
}